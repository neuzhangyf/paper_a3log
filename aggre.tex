\section{Revisit Aggregate Operation}
\label{sec:aggre}

A \emph{sequential} computation (as opposed to parallel/distributed computation) is composed of a sequence of operations on input. These operations can be classified into two categories, the aggregate operations and the non-aggregate operations. We focus on numerical aggregation in this paper. 
%In this section, we revisit aggregate operation and non-aggregate operation. We then review a specific class of computations with recursive aggregation.

\subsection{Aggregate/Non-Aggregate Operations}

%MIMD,

An \emph{\textbf{aggregate operation}} is a function $g()$ that requires more than one variables as input, i.e., $g(X)$ where $X=\{x_1, x_2, \ldots, x_n\}$, and outputs zero or more values. For example, MAX, MIN, SUM, AVG, and COUNT are commonly used aggregate operations. A special but common case is the \emph{\textbf{group-by aggregation}}. A set of input key-value pairs are grouped by key, and the values in each group are aggregated to obtain a new set of key-value pairs. In other words, the group-by aggregation is composed of multiple subset aggregate operations.
%$\{\langle k_1,x_{1,1}\rangle$, $\langle k_2,x_{2,1}\rangle$, $\langle k_1,x_{1,2}\rangle, \ldots,$ $\langle k_n,x_{n,m}\rangle\}$

\begin{comment}
\begin{equation}\label{eq:groupbyagg}
\begin{aligned}
  &g_k\big(\langle k_1,x_{1,1}\rangle,\langle k_2,x_{2,1}\rangle,\langle k_1,x_{1,2}\rangle,\ldots,\langle k_n,x_{n,m}\rangle\big)\\
  \rightarrow&\big\{\langle k_1,g(x_{1,1},x_{1,2},\ldots)\rangle,\ldots,\langle k_n,g(x_{n,1},x_{n,2},\ldots)\rangle\big\}\\
  \rightarrow&\big\{\langle k_1,y_{1}\rangle,\langle k_2,y_{2}\rangle,\ldots,\langle k_n,y_{n}\rangle\big\}.
\end{aligned}
\end{equation}
\end{comment}

On the contrary, a \emph{\textbf{non-aggregate operation}} is a function that takes only one value as input, i.e., $f(x)$, and outputs zero or more values. Since non-aggregate operation only takes one value unit as input, it has the \emph{distributive} property, i.e., $f(x\cup y)=f(x)\cup f(y)$. In essence, the aggregate operation implies a \emph{data fusion}, while a non-aggregate operation implies a \emph{data transformation}.

%For example, a decay operation on the input is a non-aggregate operation, i.e., $f(x)=\alpha\cdot x$ where $0<\alpha<1$.

Note that, the aggregate and non-aggregate operation should be judged according to the number of \emph{variable} inputs instead of constant inputs. For example, the join operation in relational algebra is in general an aggregation since it merges two data sets into one. It takes two inputs $A$ and $B$ and produces one output. However in distributed join implementation, the whole data set $B$ can be small enough to be cached on all distributed workers, which implies that $B$ is a constant input for the distributed join operations. By partitioning and distributing $A$, the join operation is achieved by merging a part of $A$ and the locally cached whole $B$ on each worker. In such a case, the join operation is considered as a non-aggregate operation because each distributed join operation only takes a part of $A$ as a variable input which is different on different processor, where $B$ is the constant for all processors.

%In parallel and distributed computing, the aggregate or non-aggregate operation should be judged from parallel and distributed computing's perspective. If an operation's inputs are from other processers or remote workers, this operation should be considered as an aggregate operation. If an operation's inputs are all locally cached, which implies that the inputs are embedded into the operation but not inputs, this operation should be considered as a non-aggregate operation. For example, the join operation in relational algebra is in general a kind of aggregation since it merges two data sets into one, i.e., it takes two inputs $A$ and $B$ and produces one output. However in distributed join implementation, suppose the whole data set $B$ is cached in each distributed worker. The join operation on each worker is then achieved by a lookup operation on the local cached data set \emph{without communication with other workers}. In such a case, the join operation only takes a part of data set $A$ as input, so that it should be considered as a non-aggregate operation.

%\subsection{Parallelization}

%In the following, we discuss the parallelization of sequential computations from the aggregation and non-aggregation's perspective.

\Paragraph{Parallelization}
Non-aggregate operation only takes one input and does not rely on any other variable, which makes it have the distributive property. By partitioning and distributing the input to many workers, the non-aggregate operations can be embarrassingly parallel \emph{without communication} between workers. Aggregate operation takes multiple inputs which may originate from other workers, so that these aggregate operations or the group-by aggregation can be parallelized but \emph{with communication} between workers. If the inputs for multiple aggregate operations are disjoint, these aggregate operations can be parallelized without communication by carefully partitioning the inputs. However, this is usually impossible since the inputs probably result from the previous computations on other workers and cannot be particularly partitioned.

%\Paragraph{Parallelizing Aggregate Operation}


%The aggregation can be parallelized only when it is decomposable. As mentioned in Sec. \ref{sec:intro}, the group-by aggregation can be parallelized since the group-by aggregate operation is actually a set of sub aggregations (each corresponding to a group specified by key). Each sub aggregation is on a key, and these sub aggregations can be executed in parallel. Alternatively, suppose the input $X=\{x_1,x_2,\ldots,x_n\}$ is divided into $m$ disjoined groups, i.e., $X=\{X_1,X_2,\ldots,X_m\}$, another kind of parallelism can be achieved when the aggregate operator is associative, i.e., $g(X_1\cup\ldots\cup X_m)=g\big(g(X_1)\cup\ldots\cup g(X_m)\big)$. The final result is the aggregation of multiple partial aggregation results, and these partial aggregations can be performed in parallel.

%\Paragraph{Sequence of Aggregate or Non-Aggregate Operations} Due to the fact that the non-aggregate operations only takes one input, a sequence of non-aggregate operations can be concatenated (i.e., $f_n\circ\ldots\circ f_1(x)=f(x)$ where $f_2\circ f_1(x)=f_2(f_1(x))$), and these concatenated $f()$ operations on different data items can be parallelized without communication. However, a sequence of aggregate operations (i.e., $g_n\circ\ldots\circ g_1(X)$) are not suggested to be concatenated and are parallelized separately, since the communication (i.e., dependencies between data items) will become more complex after concatenation.

%\Paragraph{Sequence of Interleaving Aggregate and Non-Aggregate Operations} A sequence of interleaving aggregate and non-aggregate operations with form $g_n\circ f_n\circ\ldots\circ g_1\circ f_1(x)$ should be separated by aggregations, and the concatenated two operations $g_i\circ f_i()$ can be parallelized together as long as the output of $g_i()$ operation is a single data item.


\subsection{Recursive Aggregation}

\emph{Recursive aggregation} is a sequence of interleaving aggregate and non-aggregate operations where all the aggregate operations are the same and all the non-aggregate operations are the same\footnote{There are also recursive programs without aggregations, which can be embarrassingly parallel.}. It is very common in graph algorithms, data mining and machine learning algorithms. The program with recursive aggregation is referred to as \emph{recursive program}. In this paper, we will focus on optimizing the computations with recursive aggregation.

%To simplify the analysis, we assume that the aggregate operation $g()$ is not group-by aggregation. The same analysis is also true for group-by aggregation since group-by aggregation is a set of normal aggregations. 

Let $X$ denote a set of input variables of aggregate operation and $x$ denote the input of non-aggregate operation. A recursive program can be represented as follows.
\begin{equation}
\label{eq:recursive2}
\begin{aligned}
X^{k+1}&=f(x^k),\\
x^{k+1}&=g(X^{k+1}),
\end{aligned}
\end{equation}
It starts from $x^0$, and $x^k$ is the result after $k$ recursions. It can also start from $X^0$ in which case the update order is exchanged and an aggregation is first applied. This recursive program terminates when there is no difference between $X^{k+1}$ and $X^k$ or the difference between $x^{k+1}$ and $x^k$ is small enough. Let $(g\circ f)^n$ denote $n$ applications of $(g\circ f)$. The final result of the recursive program is $(g\circ f)^n(x^0)$. Suppose $x$ contains a set of data items, the $f()$ operation can be parallelized with each working on a subset. The aggregate operation $g()$ can be parallelized if it is a group-by aggregation, but these parallel aggregate operations are synchronized to wait for the complete set of $X^{k+1}$.

\Paragraph{Example 1: SSSP} The single source shortest path (SSSP) computation is a recursive program that derives the shortest distance from a source node to all other nodes in a graph. The shortest distance is initialized as $+\infty$ for each node except for the source, which is initialized as 0. The $f()$ operation for a node $i$ takes a tuple $\langle i,d_i^k\rangle$ as input where $d_i^k$ is the shortest distance in the $k$th recursion, computes $f(d_i^k)=d_i^k+w_{i,j}=td_j^{k+1}$ for any outgoing neighbors $j$ (where $w_{i,j}$ is the distance from node $i$ to $j$), and outputs the tuples set $\{\langle j,td_j^{k+1}\rangle\}$ and its own $\langle i,d_i^k\rangle$. The aggregate operation $g()$ with respect to each node $j$ takes the input tuples $\{\langle j,td_j^{k+1}\rangle\}$, performs aggregation $g(td_j^{k+1})=min(\{td_j^{k+1}\},d_j^k)=d_j^{k+1}$, and outputs $\langle j,d_j^{k+1}\rangle$. It terminates when all nodes' shortest distances are not changed from previous recursion.

\Paragraph{Example 2: PageRank} The PageRank computation is another typical recursive program for ranking the nodes in a graph. The ranking score is initialized as $r_i^0=1/|V|$ for each node $i$ where $|V|$ is the total number of nodes. The $f()$ operation for node $i$ takes a tuple $\langle i,r_i^k\rangle$ as input where $r_i^k$ is the ranking score in the $k$th recursion, computes $f(r_i^k)=0.85*r_i^k/d_i=tr_j^{k+1}$ for any outgoing neighbors $j$ (where 0.85 is the constant damping factor and $d_i$ is the out-degree of node $i$), and outputs the tuples set $\{\langle j,tr_j^{k+1}\rangle\}$. The aggregate operation $g()$ with respect to each node $j$ takes the input tuples $\{\langle j,tr_j^{k+1}\rangle\}$, performs aggregation $g(\{tr_j^{k+1}\})=\sum_j{tr_j^{k+1}}+0.15=r_j^{k+1}$ and outputs $\langle j,r_j^{k+1}\rangle$. It terminates when the difference between two continuous recursions' ranking scores is small enough.

%For example in the PageRank algorithm,

%\Paragraph{Synchronous Aggregation}

%As stated in Sec. \ref{sec:intro}, the aggregation is the root reason of low efficiency in parallel/distributed computing. Recursive aggregation suffers the most when being parallelized due to the many synchronizations for aggregation. In the following, we will introduce asynchronous aggregation for these recursive programs to avoid the drawbacks of synchronous aggregation.





%aggregation conflict with parallelism, parallelism is only allowed between two aggregations. In order to explore another dimension of parallelism,



