\section{Related Works}
\label{sec:related}

\Paragraph{Coordination Avoidance} Minimizing coordination, or blocking communication between concurrently executing operations, is key to maximizing scalability, availability, and high performance in database systems. However, uninhibited coordination-free execution can compromise application correctness, or consistency. Coordination and consistency are the most critical issues for system performance and manageability at scale \cite{Bailis:2014:CAD:2735508.2735509}. Hellerstein et al. have set up the foundation \cite{Hellerstein:2010:DIE:1860702.1860704} and have put a lot of efforts to advance this field \cite{Alvaro:2013:CWB:2523616.2523632,Bailis:2014:QEC:2632661.2632792}. Dedalus \cite{Alvaro:2010:DDT:2185923.2185942} is proposed as a declarative foundation for the two signature features of distributed systems: mutable state, and asynchronous processing and communication. CALM (Consistent and Logical Monotonicity) principle \cite{calm} is described for reasoning about distributed system behaviour, which ensures eventual consistency by enforcing a \emph{monotonic} logic. A declarative language called Bloom \cite{Conway:2012:LLD:2391229.2391230} that encourages CALM programming and is well-suited to the inherent characteristics of distribution. Edelweiss \cite{Conway:2014:EAS:2732279.2732285} is a sublanguage of Bloom that provides an Event Log Exchange (ELE) programming model, yet automatically reclaims space without programmer assistance, which can be used to elegantly implement asynchronous communications. Blaze \cite{blaze} ensures consistent outcomes via a more efficient and manageable protocol of asynchronous point-to-point communication between producers and consumers. MacroBase \cite{Bailis:2017:MPA:3035918.3035928} is a fast data system built to explore the fast data principles that suggests asynchronously prioritizing computation on inputs that most affect outputs.


%and have put a lot of efforts to advance this field \cite{Bailis:2012:PDC:2391229.2391251,Bailis:2013:BCC:2463676.2465279,Alvaro:2013:CWB:2523616.2523632,Bailis:2014:QEC:2632661.2632792}
%CRDT \cite{Shapiro:2011:CRD:2050613.2050642} requires every update or merge operation to move forward in the monotonic semilattice. The Potential Dangers of Causal Consistency and an Explicit Solution are identified in \cite{Bailis:2012:PDC:2391229.2391251}. Bolt-on Causal Consistency \cite{Bailis:2013:BCC:2463676.2465279} reformulates the correctness criteria for causal consistency as a more general, declarative specification. In \cite{Alvaro:2013:CWB:2523616.2523632}, Alvaro et al. give a comprehensive discussion of consistency without borders. In \cite{Bailis:2014:QEC:2632661.2632792}, Bailis et al. introduce a consistency model Probabilistically Bounded Staleness (PBS).


%\cite{Alvaro:2010:DDT:2185923.2185942, Alvaro:2010:BAE:1755913.1755937, Conway:2012:LLD:2391229.2391230, Bailis:2012:PDC:2391229.2391251, Bailis:2013:ECT:2460276.2462076, Bailis:2013:BCC:2463676.2465279, Alvaro:2013:CWB:2523616.2523632, Bailis:2014:QEC:2632661.2632792, blaze, Bailis:2014:CAD:2735508.2735509, DBLP:journals/corr/GonzalezBJFHGS15}.

\Paragraph{Asynchronous Computation in Graph Analytics} Asynchronous computation has attracted much attention in the field of graph processing. GraphLab \cite{Low:2012:DGF:2212351.2212354} aims to express asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving good parallel performance. Frog \cite{8017445} is a lock-free semi-asynchronous parallel graph processing framework with a graph coloring model. Grace \cite{grace} is a single-machine parallel graph processing platform that allows customization of vertex scheduling and message selection to support asynchronous computation. Giraph++ \cite{Tian:2013:TLV:2732232.2732238} not only allows asynchronous computation while keeping the vertex-centric model but also is able to handle mutation of graphs. GiraphUC \cite{Han:2015:GUB:2777598.2777604} relies on barrierless asynchronous parallel (BAP), which reduces both message staleness and global synchronization. Maiter \cite{maiter} proposes delta-based asynchronous iterative computation model (DAIC) and supports distributed asynchronous graph processing. GunRock \cite{Wang:2016:GHG:2851141.2851145} supports fast asynchronous graph computation in GPUs. Unfortunately, the above graph systems do not support automatic asynchronization.

%Asynchronous MapReduce \cite{asynchronous} proposes to use more local synchronizations to replace global synchronizations to gain performance.

GRAPE \cite{Fan:2017:PSG:3035918.3035942} differs from prior systems in its ability to automatically parallelize existing sequential graph algorithms as a whole. Sequential graph algorithms can be "plugged into" GRAPE with minor changes, and get parallelized. As long as the sequential algorithms are correct, the GRAPE parallelization guarantees to terminate with correct answers under a monotonic condition. However, GRAPE cannot automatically asynchronize a sequential graph algorithm.

%compare with Maiter \cite{maiter}, extended to relational algebral, generalize to aggregation, support to automatically asyncronization, datalog system.

\Paragraph{Asynchronous Computation in Machine Learning} In machine learning, some algorithms with non-serializable lock-free implementations offer significant speedups. Gonzalez et al. \cite{DBLP:journals/corr/GonzalezBJFHGS15} examine the growing gap between efficient machine learning algorithms exploiting asynchrony and fine-grained communication, and commodity distributed dataflow systems (Hadoop and Spark) that are optimized for coarse-grained models. Google DeepMind group recently proposes asynchronous methods for deep reinforcement learning \cite{Mnih:2016:AMD:3045390.3045594}, which surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. DimmWitted \cite{Zhang:2014:DSM:2732977.2733001} exposes a range of options for asynchronous data sharing, which outperforms general cluster compute frameworks by orders of magnitude in the context of popular models, including SVMs, logistic regression, Gibbs sampling, and neural networks. HOGWILD! \cite{Niu:2011:HLA:2986459.2986537} is a implementation of stochastic gradient descent (SGD), which places a single copy of the model in the memory of a multi-core server and runs multiple worker processes that simultaneously run gradient steps in parallel. An asynchronous parallel stochastic coordinate descent algorithm is proposed in \cite{Liu:2015:APS:2789272.2789282}, which shows significant speedup in multi-core environment. A burgeoning cottage industry of machine learning researchers has begun to extend asynchronous execution strategies to an increasing number of optimization tasks.

\Paragraph{Datalog Systems}
Besides SociaLite \cite{Lam:2013:SDE:2510649.2511289,Seo:2013:DSD:2556549.2556572} and Myria \cite{Halperin:2014:DMB:2588555.2594530,Wang:2015:AFR:2824032.2824052}, there exist other Datalog systems. DeALS \cite{Shkapsky:2013:GQN:2536274.2536290,7113340} is a deductive database systems relying on Datalog language, supporting optimized execution over diverse platforms including sequential implementations, multi-core machines, and clusters. BigDatalog \cite{Shkapsky:2016:BDA:2882903.2915229} is built on top of Spark \cite{Zaharia:2010:SCC:1863103.1863113} and provides declarative semantics of monotonic aggregate programs. Datalography \cite{7840589} incorporates optimization techniques for efficient distributed evaluation of Datalog queries on Giraph \cite{giraph}. LogicBlox \cite{Aref:2015:DIL:2723372.2742796} is a commercial database system based on LogiQL.


%\Paragraph{Semi-Asynchronous Techniques}
%GraphLab \cite{Low:2012:DGF:2212351.2212354}
%Sequential random permutation, list contraction and tree contraction are highly parallel, dependency tree \cite{Shun:2015:SRP:2722129.2722159}. In such algorithms, instead of waiting for its turn in the sequential order, a given step can run immediately once all previous steps it depends on have been completed. The approach allows for steps to run in parallel while performing the same computations on each step as the sequential algorithm, and consequently returning the same result. \cite{Shun:2015:SRP:2722129.2722159}



%priority scheduling \cite{priori-cidr, Bailis:2017:MPA:3035918.3035928}

%Asynchronous Complex Analytics in a Distributed Dataflow Architecture \cite{DBLP:journals/corr/GonzalezBJFHGS15}





%Reducing the impact of aggregation and improving the parallelism are the fundamental issues in distributed systems. Hellerstein and his group \cite{Hellerstein:2010:DIE:1860702.1860704} have put a lot of research efforts to advance this field \cite{Alvaro:2010:DDT:2185923.2185942, Alvaro:2010:BAE:1755913.1755937, Conway:2012:LLD:2391229.2391230, Bailis:2012:PDC:2391229.2391251, Bailis:2013:ECT:2460276.2462076, Bailis:2013:BCC:2463676.2465279, Alvaro:2013:CWB:2523616.2523632, Bailis:2014:QEC:2632661.2632792, blaze, Bailis:2014:CAD:2735508.2735509, DBLP:journals/corr/GonzalezBJFHGS15}.

%\cite{Ameloot:2014:DNR:2694413.2694415}





